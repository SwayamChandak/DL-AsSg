{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"emmarex/plantdisease\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# Define image size and batch size\n",
        "IMAGE_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Load dataset using ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    \"C:/Users/suyas/.cache/kagglehub/datasets/emmarex/plantdisease/versions/1\",  # Change to your dataset path\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    \"C:/Users/suyas/.cache/kagglehub/datasets/emmarex/plantdisease/versions/1\",\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Build CNN Model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "id": "H0KQhE_-0qMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0FHy8FGssGp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the path to your dataset in Google Drive\n",
        "# Update this path according to your Google Drive structure\n",
        "base_dir = '/content/drive/MyDrive/Plant'\n",
        "\n",
        "# Define data directories\n",
        "train_dir = os.path.join(base_dir, 'Train', 'Train')\n",
        "test_dir = os.path.join(base_dir, 'Test', 'Test')\n",
        "valid_dir = os.path.join(base_dir, 'Validation', 'Validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOaM8lu9L0yR",
        "outputId": "5db070d4-a21e-461c-b8d8-b020a803ae95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "epochs = 1\n",
        "class_names = ['Healthy', 'Powdery', 'Rust']\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Print directory structure for verification\n",
        "print(\"Checking directories...\")\n",
        "print(f\"Training directory: {train_dir}\")\n",
        "print(f\"Testing directory: {test_dir}\")\n",
        "print(f\"Validation directory: {valid_dir}\")\n",
        "\n",
        "# Verify if directories exist\n",
        "for dir_path in [train_dir, test_dir, valid_dir]:\n",
        "    if not os.path.exists(dir_path):\n",
        "        raise ValueError(f\"Directory not found: {dir_path}\")\n",
        "    for class_name in class_names:\n",
        "        class_path = os.path.join(dir_path, class_name)\n",
        "        if not os.path.exists(class_path):\n",
        "            raise ValueError(f\"Class directory not found: {class_path}\")\n",
        "        num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        print(f\"Found {num_images} images in {class_path}\")\n",
        "\n",
        "# Create data generators\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "valid_test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "print(\"\\nCreating data generators...\")\n",
        "\n",
        "# Create generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    classes=class_names,\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "valid_generator = valid_test_datagen.flow_from_directory(\n",
        "    valid_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    classes=class_names,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = valid_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    classes=class_names,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"\\nCreating model...\")\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Sequential([\n",
        "    # First Convolutional Block\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=(*img_size, 3)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    # Second Convolutional Block\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    # Third Convolutional Block\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    # Dense Layers\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Learning rate scheduler\n",
        "initial_learning_rate = 0.001\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Create directories for checkpoints and logs in Google Drive\n",
        "checkpoint_dir = os.path.join(base_dir, 'checkpoints')\n",
        "logs_dir = os.path.join(base_dir, 'logs')\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        os.path.join(checkpoint_dir, 'best_model.h5'),\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy'\n",
        "    ),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=5,\n",
        "        min_lr=1e-6\n",
        "    ),\n",
        "    tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=os.path.join(logs_dir, datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
        "        histogram_freq=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=valid_generator,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\nEvaluating model...\")\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"\\nTest accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "# # Save the final model\n",
        "# final_model_path = os.path.join(base_dir, 'plant_disease_model_final.h5')\n",
        "# model.save(final_model_path)\n",
        "# print(f\"\\nModel saved to {final_model_path}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ],
      "metadata": {
        "id": "SYhPshon0GPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58413e99-e2bd-4a5b-ecb2-f32649165595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking directories...\n",
            "Training directory: /content/drive/MyDrive/Plant/Train/Train\n",
            "Testing directory: /content/drive/MyDrive/Plant/Test/Test\n",
            "Validation directory: /content/drive/MyDrive/Plant/Validation/Validation\n",
            "Found 458 images in /content/drive/MyDrive/Plant/Train/Train/Healthy\n",
            "Found 430 images in /content/drive/MyDrive/Plant/Train/Train/Powdery\n",
            "Found 434 images in /content/drive/MyDrive/Plant/Train/Train/Rust\n",
            "Found 50 images in /content/drive/MyDrive/Plant/Test/Test/Healthy\n",
            "Found 50 images in /content/drive/MyDrive/Plant/Test/Test/Powdery\n",
            "Found 50 images in /content/drive/MyDrive/Plant/Test/Test/Rust\n",
            "Found 20 images in /content/drive/MyDrive/Plant/Validation/Validation/Healthy\n",
            "Found 20 images in /content/drive/MyDrive/Plant/Validation/Validation/Powdery\n",
            "Found 20 images in /content/drive/MyDrive/Plant/Validation/Validation/Rust\n",
            "\n",
            "Creating data generators...\n",
            "Found 1322 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 150 images belonging to 3 classes.\n",
            "\n",
            "Creating model...\n",
            "\n",
            "Starting training...\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36s/step - accuracy: 0.6327 - loss: 1.3666 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mDmSzR79SYuD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}